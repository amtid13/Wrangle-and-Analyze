{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d026a7",
   "metadata": {},
   "source": [
    "# Ashley Self Project: Wrangling and Analyzing Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543b0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all libraries needed\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5057a",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f7d21",
   "metadata": {},
   "source": [
    "Below are all three pieces of data needed to complete this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b348d",
   "metadata": {},
   "source": [
    "1. Download the WeRateDogs Twitter archived data (twitter-archive-enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685bd84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Importing the twitter archived data provided by Udacity into a DataFrame\n",
    "df_archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "df_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc22a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking initial shape\n",
    "df_archive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf569a19",
   "metadata": {},
   "source": [
    "2. Use the requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96810bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the twitter image predictions tsv from Udacity\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "response.status_code    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408df8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the tsv locally\n",
    "with open('image_predictions.tsv', mode='wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a999f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the tsv into a DataFrame\n",
    "df_images_predictions = pd.read_csv('image_predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking first 5 rows for completion of prior code\n",
    "df_images_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking dataframe shape\n",
    "df_images_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc1a14",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aeef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tweepy==4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d51be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing new library and fetching tweets from Twitter API\n",
    "#I did use my API Key and Key Secret when running the code,\n",
    "#but for security purposes are not displayed here\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'API Key.....' #left blank for security purposes\n",
    "consumer_secret ='API Key Secret.....' #left blank for security purposes\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating another DataFrame for Tweet IDs\n",
    "tweet_ids = df_archive.tweet_id.values\n",
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            json.dump(tweet._json, outfile)\n",
    "            print(\"Success\")\n",
    "            outfile.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "        \n",
    "\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking length of new list\n",
    "len(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new DataFrame from the tweet_json.txt file\n",
    "json_df = pd.read_json('tweet_json.txt', lines=True, encoding='utf-8')\n",
    "json_df = json_df.rename(columns={'id': 'tweet_id'})\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd327f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling in just these four columns\n",
    "json_df = json_df[['tweet_id', 'favorite_count', 'retweet_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520f495",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Assessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b0b86",
   "metadata": {},
   "source": [
    "1. Assess the Twitter Archive file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, assess the twitter archive file visually,\n",
    "#looking for quality and tidiness\n",
    "#A summary of findings will be included later\n",
    "pd.options.display.max_colwidth = 10000\n",
    "df_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c732ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive['time_stamp'] = pd.to_datetime(df_archive['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27927b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess the data programmatically\n",
    "df_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb00217",
   "metadata": {},
   "source": [
    "We know that most of the columns with NaN are common, as most tweets in this dataset are not replies and/or retweets, however, we should still work to drop all retweets to clean up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at expanded_urls to see if NaN values are retweets or original tweets\n",
    "df_archive[df_archive['expanded_urls'].isnull()].loc[:, ['expanded_urls',\n",
    "                                                         'in_reply_to_status_id',\n",
    "                                                         'in_reply_to_user_id',\n",
    "                                                         'retweeted_status_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cac2b6",
   "metadata": {},
   "source": [
    "We can see in the code above that only four expanded_urls with NaN are original tweets, and not replies/retweets (185, 375, 707, 1445)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660903ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By expanding these four tweets, we see that the first one (185) is a retweet by the RT@ in the text column - issue found!\n",
    "url_nan_original_tweets = df_archive.iloc[[185, 375, 707, 1445], :].tweet_id\n",
    "df_archive.iloc[[185, 375, 707, 1445], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666b72c",
   "metadata": {},
   "source": [
    "#### Quality and Tidiness Issues - df_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7bc9f",
   "metadata": {},
   "source": [
    "##### Quality\n",
    "1. Too many retweets present in the dataframe, we want original tweets only\n",
    "2. The text column also contains text followed by a url, this should be cleaned up and either separated, or condensed to just text\n",
    "3. Need to change data type from int to string for tweet_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e6ccf",
   "metadata": {},
   "source": [
    "##### Tidiness\n",
    "1. The last four columns: doggo, floofer, pupper, puppo can be combined into one column with a filter to condense and clean up the data\n",
    "2. Spitting the timestamp column would be helpful - separate them into two columns: date and time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499df60",
   "metadata": {},
   "source": [
    "2. Assess the Image Predictions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5919bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, assess the image predictions file visually,\n",
    "#looking for quality and tidiness\n",
    "#A summary of findings will be included later\n",
    "df_images_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe172f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess the data programmatically\n",
    "df_images_predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df007ebb",
   "metadata": {},
   "source": [
    "Through programmatic assessing in the code above, we see that no columns have NaN values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46509ba",
   "metadata": {},
   "source": [
    "There is no data that seems out of place with the descriptive statistics of this df or the sample above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_images_predictions.duplicated(subset=['jpg_url']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dae43",
   "metadata": {},
   "source": [
    "#### Quality and Tidiness Issues - df_images_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca172b",
   "metadata": {},
   "source": [
    "##### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de8ce4",
   "metadata": {},
   "source": [
    "1. There is a mix of capitalized and non-capitalized dog breeds in multiple columns (needs consistency)\n",
    "2. Multiple columns have non-descriptive names (ie: p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog)\n",
    "3. Need to drop the 66 duplicates in the jpg_url column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb32d15",
   "metadata": {},
   "source": [
    "3. Assess the Tweet json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, assess the image predictions file visually,\n",
    "#looking for quality and tidiness\n",
    "#A summary of findings will be included later\n",
    "json_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess the data programmatically\n",
    "json_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f29f84",
   "metadata": {},
   "source": [
    "Through programmatic assessing in the code above, we see that no columns have NaN values, too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669238d",
   "metadata": {},
   "source": [
    "#### Quality and Tidiness Issues - json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744268d",
   "metadata": {},
   "source": [
    "##### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8351f06",
   "metadata": {},
   "source": [
    "1. Too many retweets present in the dataframe, we want original tweets only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a824ef0",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c102d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of all three datasets\n",
    "df_archive_clean = df_archive.copy()\n",
    "df_images_predictions_clean = df_images_predictions.copy()\n",
    "json_df_clean = json_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ef161",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:\n",
    "1. & 2. Too many retweets present in the dataframe, we want original tweets only (with images) - addresses two quality issues\n",
    "            Cleans both NaN values and deletes unecessary columns with retweeted info and replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72307523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean[pd.isnull(df_archive_clean['retweeted_status_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dd2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing code to make sure it worked\n",
    "print(sum(df_archive_clean.retweeted_status_id.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0795fab",
   "metadata": {},
   "source": [
    "Filtered/deleted all retweeted information through above code - works by showing retweeted columns as 0 counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8421d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of columns related to retweets and replies\n",
    "df_archive_clean = df_archive_clean.drop(['retweeted_status_id',\n",
    "                                          'retweeted_status_user_id',\n",
    "                                          'retweeted_status_timestamp',\n",
    "                                          'in_reply_to_status_id',\n",
    "                                          'in_reply_to_user_id'],\n",
    "                                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b543faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure it worked\n",
    "df_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8d464",
   "metadata": {},
   "source": [
    "Works - issues resolved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a101a7f",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e47c1",
   "metadata": {},
   "source": [
    "3. The text column also contains text followed by a url, this should be cleaned up and either separated, or condensed to just text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9675d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaned output will be stored in a list\n",
    "def new_list(dataframe, column, word):\n",
    "    text_list = []\n",
    "    for text in dataframe[column]:\n",
    "        text_list.append(text)\n",
    "    clean_text_list = []\n",
    "    for text_2 in text_list:\n",
    "        clean_text = text_2[:text_2.find(word)-1]\n",
    "        clean_text_list.append(clean_text)\n",
    "    return(clean_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14146b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column with the clean text list to make sure the code works\n",
    "df_archive_clean['new_text'] = new_list(df_archive_clean, 'text', 'https')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View old and new column to make sure all code works\n",
    "df_archive_clean.loc[:, ['text', 'new_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db4a0e",
   "metadata": {},
   "source": [
    "It worked! Now lets input the new column into the old column, overriding the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d918244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['text'] = new_list(df_archive_clean, 'text', 'https')\n",
    "df_archive_clean = df_archive_clean.drop(['new_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_archive_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135939b6",
   "metadata": {},
   "source": [
    "Viola - it worked! Issue resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc40c7",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b876cb",
   "metadata": {},
   "source": [
    "4. Need to change data type from int to string for tweet_id column in all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be24fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['tweet_id'] = df_archive_clean['tweet_id'].astype('str')\n",
    "df_images_predictions_clean['tweet_id'] = df_images_predictions_clean['tweet_id'].astype('str')\n",
    "json_df_clean['tweet_id'] = json_df_clean['tweet_id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_archive_clean.info()\n",
    "df_images_predictions_clean.info()\n",
    "json_df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bebf23",
   "metadata": {},
   "source": [
    "Works - issue resolved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3603a61",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a8cfb",
   "metadata": {},
   "source": [
    "5. After looking at the data programmatically, we find that some of the urls in expanded_urls are missing\n",
    "            *We can look at tweet_id to possibly get the missing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bd8f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_urls(row):\n",
    "    if pd.notnull(row['expanded_urls']):\n",
    "        return row\n",
    "    else:\n",
    "        tweet_id = row['tweet_id']\n",
    "        row['expanded_urls'] = 'https://twitter.com/dog_rates/status/{}'.format(tweet_id)\n",
    "        return row\n",
    "\n",
    "df_archive_clean = df_archive_clean.apply(add_urls, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec20330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_archive_clean[df_archive_clean['expanded_urls'].isnull()]\n",
    "df_archive_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35715e",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb38614",
   "metadata": {},
   "source": [
    "6. There is a mix of capitalized and non-capitalized dog breeds in multiple columns (needs consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed0cda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions_clean['p1'] = df_images_predictions_clean['p1'].str.title()\n",
    "df_images_predictions_clean['p2'] = df_images_predictions_clean['p2'].str.title()\n",
    "df_images_predictions_clean['p3'] = df_images_predictions_clean['p3'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddddaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_images_predictions_clean.loc[:,['p1', 'p2', 'p3']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379a302",
   "metadata": {},
   "source": [
    "Works - issue resolved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc7c80",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5afaac",
   "metadata": {},
   "source": [
    "7. Multiple columns have non-descriptive names (ie: p1, p1_conf, p1_dog, p2, p2_conf, p2_dog, p3, p3_conf, p3_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "471fcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions_clean = df_images_predictions_clean.rename(columns={'jpg_url': 'image_url',\n",
    "                                                                  'img_num': 'image_number',\n",
    "                                                                 'p1': 'top_prediction',\n",
    "                                                                  'p1_conf': 'top_prediction_confidence',\n",
    "                                                                  'p2': 'second_prediction',\n",
    "                                                                  'p2_conf': 'second_prediction_confidence',\n",
    "                                                                  'p3': 'third_prediction',\n",
    "                                                                  'p3_conf': 'third_prediction_confidence',\n",
    "                                                                  'p1_dog': 'top_dog_prediction',\n",
    "                                                                  'p2_dog': 'second_dog_prediction',\n",
    "                                                                  'p3_dog': 'third_dog_prediction',\n",
    "                                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_images_predictions_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2e972",
   "metadata": {},
   "source": [
    "Works - issue resolved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66feab",
   "metadata": {},
   "source": [
    "###### Address Quality Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ade0cc",
   "metadata": {},
   "source": [
    "8. Need to drop the 66 duplicates in the jpg_url column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbfee14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_predictions_clean = df_images_predictions_clean.drop_duplicates(subset = ['image_url'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "sum(df_images_predictions_clean['image_url'].duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444be7b",
   "metadata": {},
   "source": [
    "Works - issue resolved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86128b",
   "metadata": {},
   "source": [
    "###### Address Tidiness Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799abb45",
   "metadata": {},
   "source": [
    "1. The last four columns: doggo, floofer, pupper, puppo can be combined into one column with a filter to condense and clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5018c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['doggo'].replace('None', '', inplace=True)\n",
    "df_archive_clean['floofer'].replace('None', '', inplace=True)\n",
    "df_archive_clean['pupper'].replace('None', '', inplace=True)\n",
    "df_archive_clean['puppo'].replace('None', '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ddabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['level_of_dog'] = (df_archive_clean['doggo'] +\n",
    "                                    df_archive_clean['floofer'] +\n",
    "                                    df_archive_clean['pupper'] +\n",
    "                                    df_archive_clean['puppo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7e6d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['level_of_dog'].replace('', 'NaN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_archive_clean['level_of_dog'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4922bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean.loc[df_archive_clean.level_of_dog == 'doggopupper'] = 'doggo, pupper'\n",
    "df_archive_clean.loc[df_archive_clean.level_of_dog == 'doggopuppo'] = 'doggo, puppo'\n",
    "df_archive_clean.loc[df_archive_clean.level_of_dog == 'doggofloofer'] = 'doggo, floofer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b802d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['level_of_dog'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdb064",
   "metadata": {},
   "source": [
    "Works - fixed Udacity Reviewers required change for level of dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean.info()\n",
    "df_archive_clean.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ed803",
   "metadata": {},
   "source": [
    "Works - issue resolved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d203934",
   "metadata": {},
   "source": [
    "###### Address Tidiness Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e73e5",
   "metadata": {},
   "source": [
    "2. Spitting the timestamp column would be helpful and make the data easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ed826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['temp_time'] = pd.DatetimeIndex(df_archive_clean['timestamp']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "bf2f7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting date, year, month\n",
    "temp_time = pd.DatetimeIndex(df_archive_clean['timestamp'])\n",
    "\n",
    "df_archive_clean['Date'] = temp_time.date\n",
    "df_archive_clean['Year'] = temp_time.year\n",
    "df_archive_clean['Month'] = temp_time.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fff6b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substituting month name for number\n",
    "df_archive_clean['Month'].replace(1, 'January', inplace=True)\n",
    "df_archive_clean['Month'].replace(2, 'February', inplace=True)\n",
    "df_archive_clean['Month'].replace(3, 'March', inplace=True)\n",
    "df_archive_clean['Month'].replace(4, 'April', inplace=True)\n",
    "df_archive_clean['Month'].replace(5, 'May', inplace=True)\n",
    "df_archive_clean['Month'].replace(6, 'June', inplace=True)\n",
    "df_archive_clean['Month'].replace(7, 'July', inplace=True)\n",
    "df_archive_clean['Month'].replace(8, 'August', inplace=True)\n",
    "df_archive_clean['Month'].replace(9, 'September', inplace=True)\n",
    "df_archive_clean['Month'].replace(10, 'October', inplace=True)\n",
    "df_archive_clean['Month'].replace(11, 'November', inplace=True)\n",
    "df_archive_clean['Month'].replace(12, 'December', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fdc5eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['Day_of_Week'] = temp_time.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a8b4536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substituting day of week name for number\n",
    "df_archive_clean['Day_of_Week'].replace(0, 'Monday', inplace=True)\n",
    "df_archive_clean['Day_of_Week'].replace(1, 'Tuesday', inplace=True)\n",
    "df_archive_clean['Day_of_Week'].replace(2, 'Wednesday', inplace=True)\n",
    "df_archive_clean['Day_of_Week'].replace(3, 'Thursday', inplace=True)\n",
    "df_archive_clean['Day_of_Week'].replace(4, 'Friday', inplace=True)\n",
    "df_archive_clean['Day_of_Week'].replace(5, 'Saturday', inplace=True)\n",
    "df_archive_clean['Day_of_Week'].replace(6, 'Sunday', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6bdac18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['Time'] = temp_time.time\n",
    "df_archive_clean['Hour'] = temp_time.hour\n",
    "\n",
    "df_archive_clean = df_archive_clean.drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure code works\n",
    "df_archive_clean.info()\n",
    "df_archive_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c2ea8",
   "metadata": {},
   "source": [
    "### Merge all tables into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge archive and image\n",
    "twitter_1 = pd.merge(df_archive_clean, \n",
    "                     df_images_predictions_clean, \n",
    "                     how = 'left', on = 'tweet_id')\n",
    "\n",
    "#keep rows that have image_url\n",
    "twitter_1 = twitter_1[twitter_1['image_url'].notnull()]\n",
    "\n",
    "#Check to make sure code works\n",
    "twitter_1.info()\n",
    "twitter_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge twitter_1 and json\n",
    "twitter_2 = pd.merge(twitter_1, json_df_clean, \n",
    "                      how = 'left', on = 'tweet_id')\n",
    "\n",
    "#Check to make sure code works\n",
    "twitter_2.info()\n",
    "twitter_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3a1617ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we store the newly merged dataset in twitter_archive_master.csv\n",
    "twitter_2.to_csv('twitter_archive_master.csv', \n",
    "                 index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151d33d",
   "metadata": {},
   "source": [
    "### Analyze and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7a328",
   "metadata": {},
   "source": [
    "__First Insight__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31794a",
   "metadata": {},
   "source": [
    "What is the start and finish date of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1bec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_2['Date'][-1:][len(twitter_2)-1])\n",
    "print(twitter_2['Date'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9438fa8",
   "metadata": {},
   "source": [
    "- The dataset contains data from __11-15-2015__ through __08-01-2017__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d19b8a",
   "metadata": {},
   "source": [
    "__Second Insight__ - two questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba3566",
   "metadata": {},
   "source": [
    "What is the most common dog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d394c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_2['top_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121387e",
   "metadata": {},
   "source": [
    "__Golden Retriever__ is the most common dog in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f0043",
   "metadata": {},
   "source": [
    "What are the top 5 most common dog names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3256acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_2['name'].value_counts()[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4e7d1",
   "metadata": {},
   "source": [
    "__Charlie__ is the most common dog name in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a31df",
   "metadata": {},
   "source": [
    "__Third Insight__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3af2a",
   "metadata": {},
   "source": [
    "Let's look at the descriptive statistics of the overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25860a87",
   "metadata": {},
   "source": [
    "- The __mean rating__ for this dataset is __12.3__.\n",
    "- On average, __7,017__ twitter users __favorited__ each WeRateDogs original tweets.\n",
    "- However, the __average retweet count__ is significantly lower at only __2,006__ retweets.\n",
    "- The tweet with the __highest likes__ had __142,045__ likes.\n",
    "- The __most retweeted tweet__ had __69,593__ retweets.\n",
    "- The tweet with the __lowest amount of likes__ only had __64__ likes.\n",
    "- The __least retweeted tweet__ only had __11__ retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_2.groupby(['Month', 'Year'], \n",
    "                               sort=False).count().iloc[::-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b1b7fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_month = twitter_2.groupby(['Month', 'Year'], \n",
    "                               sort=False).count().iloc[::-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.set_style(\"white\")\n",
    "tweets_per_month.plot(kind='bar', color='b')\n",
    "ax.set_title('Number of original tweets per month', fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylabel('count', fontsize=12)\n",
    "ax.set_xlabel('')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b05bd",
   "metadata": {},
   "source": [
    "__November 2015__ and __December 2015__ were the busiest months for WeRateDogs twitter account with the number of original posted tweets, both exceeding 250. From __April 2016__ to __August 2017__, every month had less than 100 tweets total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89805ae",
   "metadata": {},
   "source": [
    "Some code has been adapted from:\n",
    "- https://github.com/MrGeislinger/UdacityDAND_Proj_WrangleAndAnalyzeData\n",
    "- https://github.com/latinacode/Wrangle-and-Analyze-Data\n",
    "- https://github.com/NTavou/Wrangle_and_Analyze_Twitter_Data\n",
    "- https://github.com/gouravaich/wrangle-analyze-weratedogs-twitter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
